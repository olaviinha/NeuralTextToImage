{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAION-400M.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1HyosTLhmGaRNHh3GAoe1RjzwawEJwSJR",
      "authorship_tag": "ABX9TyPpXLLSSEYsv6hcq884S151",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralTextToImage/blob/main/LAION_400M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCskRiNym_LW"
      },
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Big Sleep: LAION 400M <font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Neural text-to-image</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralTextToImage\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "Latent Diffusion LAION 400M. Notebook based on [@multimodalart's notebook](https://colab.research.google.com/github/multimodalart/latent-diffusion-notebook/blob/main/Latent_Diffusion_LAION_400M_model_text_to_image.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6-8KXU6m8TM",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "#@markdown <small>Mounting Drive will enable this notebook to save outputs directly to your Drive. Otherwise you will need to copy/download them manually from this notebook.</small>\n",
        "\n",
        "force_setup = False\n",
        "pip_packages = ''\n",
        "main_repository = ''\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown <small>`local_models_path` is optional. It's beneficial and reduces setup time significantly if you use the notebook again in the future.</small>\n",
        "local_models_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Download the repo from Github\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive is True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  create_dirs(['/content/faux_drive'])\n",
        "  drive_root = '/content/faux_drive/'\n",
        "\n",
        "if main_repository is not '':\n",
        "  !git clone {main_repository}\n",
        "\n",
        "notebook_models = \"/content/latent-diffusion/models/ldm/text2img-large\"\n",
        "model_path = fix_path(drive_root+local_models_path) if local_models_path is not '' else notebook_models\n",
        "\n",
        "#@title Installation\n",
        "!git clone https://github.com/crowsonkb/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!pip install transformers\n",
        "!pip install open_clip_torch\n",
        "!pip install autokeras\n",
        "!pip install tensorflow\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan \n",
        "\n",
        "#@title Download model\n",
        "%cd /content/latent-diffusion\n",
        "\n",
        "if not os.path.isdir(model_path):\n",
        "  os.mkdir(model_path)\n",
        "if not os.path.isfile(f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\"):\n",
        "  !wget -O $model_path/latent_diffusion_txt2img_f8_large.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n",
        "\n",
        "\n",
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "#@title Import stuff\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "tqdm_auto_model = __import__(\"tqdm.auto\", fromlist=[None]) \n",
        "sys.modules['tqdm'] = tqdm_auto_model\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from open_clip import tokenizer\n",
        "import open_clip\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "#@title Load necessary functions\n",
        "\n",
        "\n",
        "def load_safety_model(clip_model):\n",
        "    \"\"\"load the safety model\"\"\"\n",
        "    import autokeras as ak  # pylint: disable=import-outside-toplevel\n",
        "    from tensorflow.keras.models import load_model  # pylint: disable=import-outside-toplevel\n",
        "    from os.path import expanduser  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "    home = expanduser(\"~\")\n",
        "\n",
        "    cache_folder = home + \"/.cache/clip_retrieval/\" + clip_model.replace(\"/\", \"_\")\n",
        "    if clip_model == \"ViT-L/14\":\n",
        "        model_dir = cache_folder + \"/clip_autokeras_binary_nsfw\"\n",
        "        dim = 768\n",
        "    elif clip_model == \"ViT-B/32\":\n",
        "        model_dir = cache_folder + \"/clip_autokeras_nsfw_b32\"\n",
        "        dim = 512\n",
        "    else:\n",
        "        raise ValueError(\"Unknown clip model\")\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(cache_folder, exist_ok=True)\n",
        "\n",
        "        from urllib.request import urlretrieve  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "        path_to_zip_file = cache_folder + \"/clip_autokeras_binary_nsfw.zip\"\n",
        "        if clip_model == \"ViT-L/14\":\n",
        "            url_model = \"https://raw.githubusercontent.com/LAION-AI/CLIP-based-NSFW-Detector/main/clip_autokeras_binary_nsfw.zip\"\n",
        "        elif clip_model == \"ViT-B/32\":\n",
        "            url_model = (\n",
        "                \"https://raw.githubusercontent.com/LAION-AI/CLIP-based-NSFW-Detector/main/clip_autokeras_nsfw_b32.zip\"\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model {}\".format(clip_model))\n",
        "        urlretrieve(url_model, path_to_zip_file)\n",
        "        import zipfile  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "        with zipfile.ZipFile(path_to_zip_file, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(cache_folder)\n",
        "\n",
        "    loaded_model = load_model(model_dir, custom_objects=ak.CUSTOM_OBJECTS)\n",
        "    loaded_model.predict(np.random.rand(10 ** 3, dim).astype(\"float32\"), batch_size=10 ** 3)\n",
        "\n",
        "    return loaded_model\n",
        "\n",
        "def is_unsafe(safety_model, embeddings, threshold=0.5):\n",
        "    \"\"\"find unsafe embeddings\"\"\"\n",
        "    nsfw_values = safety_model.predict(embeddings, batch_size=embeddings.shape[0])\n",
        "    x = np.array([e[0] for e in nsfw_values])\n",
        "    #print(x)\n",
        "    return True if x > threshold else False\n",
        "#NSFW CLIP Filter\n",
        "safety_model = load_safety_model(\"ViT-B/32\")\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\") \n",
        "model = load_model_from_config(config, f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "def run(opt):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if opt.plms:\n",
        "        opt.ddim_eta = 0\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    \n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "    prompt = opt.prompt\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            with model.ema_scope():\n",
        "                uc = None\n",
        "                if opt.scale > 0:\n",
        "                    uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                    shape = [4, opt.H//8, opt.W//8]\n",
        "                    samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                    conditioning=c,\n",
        "                                                    batch_size=opt.n_samples,\n",
        "                                                    shape=shape,\n",
        "                                                    verbose=False,\n",
        "                                                    unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,\n",
        "                                                    eta=opt.ddim_eta)\n",
        "\n",
        "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                    x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    for x_sample in x_samples_ddim:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        image_vector = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                        image = preprocess(image_vector).unsqueeze(0)\n",
        "                        with torch.no_grad():\n",
        "                          image_features = clip_model.encode_image(image)\n",
        "                        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                        query = image_features.cpu().detach().numpy().astype(\"float32\")\n",
        "                        unsafe = is_unsafe(safety_model,query,opt.nsfw_threshold)\n",
        "                        if(not unsafe):\n",
        "                          image_vector.save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                        else:\n",
        "                          raise Exception('Potential NSFW content was detected on your outputs. Try again with different prompts. If you feel your prompt was not supposed to give NSFW outputs, this may be due to a bias in the model')\n",
        "                        base_count += 1\n",
        "                    all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "    # additionally, save as grid\n",
        "    grid = torch.stack(all_samples, 0)\n",
        "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "    grid = make_grid(grid, nrow=opt.n_samples)\n",
        "\n",
        "    # to image\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
        "    display(Image.fromarray(grid.astype(np.uint8)))\n",
        "    #print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")\n",
        "\n",
        "\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "op(c.ok, 'Setup finished.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "H2z1os4Bocdy"
      },
      "source": [
        "#@title Run\n",
        "import argparse\n",
        "generate_image_of = \"\" #@param{type:\"string\"}\n",
        "output_dir = \"\" #@param{type:\"string\"}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "ETA = 0 #@param{type:\"integer\"}\n",
        "iterations = 2 #@param{type:\"integer\"}\n",
        "width=256 #@param{type:\"slider\", min:256, max:640, step:64}\n",
        "height=384 #@param{type:\"slider\", min:256, max:640, step:64}\n",
        "samples_in_parallel=1 #@param{type:\"slider\", min:1, max:3}\n",
        "diversity_scale=5.0 #@param {type:\"number\"}\n",
        "PLMS_sampling=True #@param {type:\"boolean\"}\n",
        "\n",
        "outputs_path = fix_path(drive_root+output_dir)\n",
        "if not os.path.isdir(outputs_path):\n",
        "  os.mkdir(outputs_path)\n",
        "\n",
        "if ';' in generate_image_of:\n",
        "  prompts = generate_image_of.split(';')\n",
        "else:\n",
        "  prompts = [generate_image_of]\n",
        "\n",
        "if '://' in generate_image_of:\n",
        "  iterator = range(0,100)\n",
        "  use_api = True\n",
        "else:\n",
        "  iterator = prompts\n",
        "  use_api = False\n",
        "\n",
        "for i, Prompt in enumerate(iterator):\n",
        "  args = argparse.Namespace(\n",
        "      prompt = Prompt, \n",
        "      outdir=f'{outputs_path}',\n",
        "      ddim_steps = steps,\n",
        "      ddim_eta = ETA,\n",
        "      n_iter = iterations,\n",
        "      W=width,\n",
        "      H=height,\n",
        "      n_samples=samples_in_parallel,\n",
        "      scale=diversity_scale,\n",
        "      plms=PLMS_sampling,\n",
        "      nsfw_threshold=999\n",
        "  )\n",
        "  run(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}