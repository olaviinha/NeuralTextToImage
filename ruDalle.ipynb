{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ruDalle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgBxyNeMWyFeyv0EulLT2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralTextToImage/blob/main/ruDalle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Big Sleep: ruDALLE <font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Neural text-to-image</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralImageGeneration\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "ruDALLE generates images from text input. This notebook is based on [FractalLibrary's ruDALL-E Mass Batcher Beta notebook](https://colab.research.google.com/github/FractalLibrary/ruDALL-E/blob/main/ruDALL_E_Mass_Batcher.ipynb). You may queue infinite text prompts by separating them by semicolon (`;`).\n"
      ],
      "metadata": {
        "id": "sRio5K49gYrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JwFG7MVmvnUC"
      },
      "outputs": [],
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "\n",
        "force_setup = False\n",
        "pip_packages = ''\n",
        "main_repository = ''\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Download the repo from Github\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive is True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  create_dirs(['/content/faux_drive'])\n",
        "  drive_root = '/content/faux_drive/'\n",
        "\n",
        "if main_repository is not '':\n",
        "  !git clone {main_repository}\n",
        "\n",
        "\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "!pip install rudalle==0.0.1rc7 > /dev/null\n",
        "!pip install mtranslate > /dev/null\n",
        "\n",
        "from mtranslate import translate\n",
        "from rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_clip\n",
        "from rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan, get_ruclip\n",
        "from rudalle.utils import seed_everything\n",
        "\n",
        "# %%time\n",
        "device = 'cuda'\n",
        "dalle = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device)\n",
        "# %%time\n",
        "try:\n",
        "    realesrgan, tokenizer, ruclip, ruclip_processor\n",
        "except NameError:\n",
        "    realesrgan = get_realesrgan('x4', device=device)\n",
        "    tokenizer = get_tokenizer()\n",
        "    vae = get_vae(dwt=True).to(device) \n",
        "    ruclip, ruclip_processor = get_ruclip('ruclip-vit-base-patch32-v5')\n",
        "    ruclip = ruclip.to(device)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import imageio\n",
        "\n",
        "from rudalle import utils\n",
        "\n",
        "\n",
        "def generate_images(text, tokenizer, dalle, vae, top_k, top_p, images_num, temperature=1.0, bs=8, seed=None,\n",
        "                    use_cache=True):\n",
        "    if seed is not None:\n",
        "        utils.seed_everything(seed)\n",
        "\n",
        "    vocab_size = dalle.get_param('vocab_size')\n",
        "    text_seq_length = dalle.get_param('text_seq_length')\n",
        "    image_seq_length = dalle.get_param('image_seq_length')\n",
        "    total_seq_length = dalle.get_param('total_seq_length')\n",
        "    device = dalle.get_param('device')\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    input_ids = tokenizer.encode_text(text, text_seq_length=text_seq_length)\n",
        "    pil_images, scores = [], []\n",
        "    for chunk in more_itertools.chunked(range(images_num), bs):\n",
        "        chunk_bs = len(chunk)\n",
        "        with torch.no_grad():\n",
        "            attention_mask = torch.tril(torch.ones((chunk_bs, 1, total_seq_length, total_seq_length), device=device))\n",
        "            out = input_ids.unsqueeze(0).repeat(chunk_bs, 1).to(device)\n",
        "            has_cache = False\n",
        "            sample_scores = []\n",
        "            for i in tqdm(range(len(input_ids), total_seq_length)):\n",
        "                logits, has_cache = dalle(out[:, :i], attention_mask,\n",
        "                                          has_cache=has_cache, use_cache=use_cache, return_loss=False)\n",
        "                logits = logits[:, -1, vocab_size:]\n",
        "                logits /= temperature\n",
        "                filtered_logits = transformers.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "                probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "                sample = torch.multinomial(probs, 1)\n",
        "                sample_scores.append(probs[torch.arange(probs.size(0)), sample.transpose(0, 1)])\n",
        "                out = torch.cat((out, sample), dim=-1)\n",
        "            codebooks = out[:, -image_seq_length:]\n",
        "            images = vae.decode(codebooks)\n",
        "            pil_images += utils.torch_tensors_to_pil_list(images)\n",
        "            scores += torch.cat(sample_scores).sum(0).detach().cpu().numpy().tolist()\n",
        "    return pil_images, scores\n",
        "\n",
        "\n",
        "def show(pil_images, nrow=4, filename = None):\n",
        "    imgs = torchvision.utils.make_grid(utils.pil_list_to_torch_tensors(pil_images), nrow=nrow)\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs.cpu()]\n",
        "    fig = None\n",
        "    axs = None\n",
        "    if filename is None:\n",
        "      fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(14, 14))\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = torchvision.transforms.functional.to_pil_image(img)\n",
        "        if filename is not None:\n",
        "          imageio.imwrite(filename, np.array(img))\n",
        "        else:\n",
        "          axs[0, i].imshow(np.asarray(img))\n",
        "          axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    if filename is None:\n",
        "      fig.show()\n",
        "      plt.show()\n",
        "    \n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rudalle.dalle.utils import divide, split_tensor_along_last_dim\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def gelu_impl(x):\n",
        "    \"\"\"OpenAI's gelu implementation.\"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x * (1.0 + 0.044715 * x * x)))\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return gelu_impl(x)\n",
        "\n",
        "\n",
        "def dalle_layer_forward(self, hidden_states, ltor_mask, has_cache, use_cache):\n",
        "    # hidden_states: [b, s, h]\n",
        "    # ltor_mask: [1, 1, s, s]\n",
        "\n",
        "    # Layer norm at the begining of the transformer layer.\n",
        "    # output = hidden_states\n",
        "    # att_has_cache, mlp_has_cache = True, True\n",
        "    layernorm_output = self.input_layernorm(hidden_states)\n",
        "\n",
        "    # Self attention.\n",
        "    attention_output, att_has_cache = self.attention(\n",
        "        layernorm_output, ltor_mask, has_cache=has_cache, use_cache=use_cache)  # if False else layernorm_output, True\n",
        "\n",
        "    if self.cogview_sandwich_layernorm:\n",
        "        attention_output = self.before_first_addition_layernorm(\n",
        "            attention_output, has_cache=has_cache, use_cache=use_cache)\n",
        "\n",
        "    # Residual connection.\n",
        "    layernorm_input = hidden_states + attention_output\n",
        "\n",
        "    # Layer norm post the self attention.\n",
        "    layernorm_output = self.post_attention_layernorm(\n",
        "        layernorm_input, has_cache=has_cache, use_cache=use_cache)\n",
        "\n",
        "    # MLP.\n",
        "    # mlp_has_cache = True\n",
        "    mlp_output, mlp_has_cache = self.mlp(\n",
        "        layernorm_output, has_cache=has_cache, use_cache=use_cache\n",
        "        )  # if False else layernorm_output, True\n",
        "\n",
        "    if self.cogview_sandwich_layernorm:\n",
        "        mlp_output = self.before_second_addition_layernorm(\n",
        "            mlp_output, has_cache=has_cache, use_cache=use_cache)\n",
        "\n",
        "    # Second residual connection.\n",
        "    output = layernorm_input + mlp_output\n",
        "\n",
        "    return output, att_has_cache and mlp_has_cache\n",
        "\n",
        "\n",
        "# About 1.3x speedup. Query/key/value cat is surprisingly fast.\n",
        "def dalle_sa_forward(self, hidden_states, ltor_mask, has_cache=False, use_cache=False,):\n",
        "    # hidden_states: [b, s, h]\n",
        "    # ltor_mask: [1, 1, s, s]\n",
        "    # Attention heads. [b, s, hp]\n",
        "    \n",
        "    def calculate_attention_scores(query_layer, key_layer, ltor_mask):\n",
        "        key_t = key_layer.transpose(-1, -2)\n",
        "        if self.cogview_pb_relax:\n",
        "            attention_scores = torch.matmul(\n",
        "                query_layer / math.sqrt(self.hidden_size_per_attention_head),\n",
        "                key_t\n",
        "            )\n",
        "        else:\n",
        "            attention_scores = torch.matmul(query_layer, key_t) / math.sqrt(self.hidden_size_per_attention_head)\n",
        "\n",
        "        ltor_mask = ltor_mask[:, :, -attention_scores.shape[-2]:]\n",
        "\n",
        "        attention_scores = torch.mul(attention_scores, ltor_mask) - 10000.0 * (1.0 - ltor_mask)\n",
        "        if self.cogview_pb_relax:\n",
        "            # normalize attention scores. Should not affect resulting softmax value\n",
        "            alpha = 32\n",
        "            attention_scores_scaled = attention_scores / alpha\n",
        "            attention_scores_scaled_maxes, _ = attention_scores_scaled.detach().view(\n",
        "                [attention_scores.size(0), attention_scores.size(1), -1]\n",
        "            ).max(dim=-1)  # max per head per sample\n",
        "            attention_scores_scaled_maxes = attention_scores_scaled_maxes.unsqueeze(-1).unsqueeze(-1).expand(\n",
        "                [-1, -1, attention_scores.size(2), attention_scores.size(3)]\n",
        "            )  # expand to [b, np, s, s]\n",
        "            attention_scores = (attention_scores_scaled - attention_scores_scaled_maxes) * alpha\n",
        "        return attention_scores\n",
        "    \n",
        "    t = hidden_states.shape[-2]\n",
        "    if has_cache and use_cache:\n",
        "        mixed_x_layer = self.query_key_value(hidden_states[:, self.past_output.shape[-2]:, :])\n",
        "    else:\n",
        "        mixed_x_layer = self.query_key_value(hidden_states)\n",
        "\n",
        "    (mixed_query_layer,\n",
        "        mixed_key_layer,\n",
        "        mixed_value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n",
        "\n",
        "    query_layer = self._transpose_for_scores(mixed_query_layer)\n",
        "    key_layer = self._transpose_for_scores(mixed_key_layer)\n",
        "    value_layer = self._transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "    if use_cache and has_cache:\n",
        "        value_layer = torch.cat((self.past_value, value_layer), dim=-2)\n",
        "        key_layer = torch.cat((self.past_key, key_layer), dim=-2)\n",
        "    attention_scores = calculate_attention_scores(\n",
        "        query_layer=query_layer, key_layer=key_layer, ltor_mask=ltor_mask\n",
        "    )\n",
        "\n",
        "    if use_cache:\n",
        "        self.past_key = key_layer\n",
        "        self.past_value = value_layer\n",
        "    else:\n",
        "        has_cache = False\n",
        "\n",
        "    if use_cache and has_cache:\n",
        "        attention_scores = attention_scores[..., -1:, :]\n",
        "    \n",
        "    # Attention probabilities. [b, np, s, s]\n",
        "    attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = self.attention_dropout(attention_probs)\n",
        "\n",
        "    # Context layer.\n",
        "    # [b, np, s, hn]\n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    # [b, s, np, hn]\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size,)\n",
        "    # [b, s, hp]\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    # Output. [b, s, h]\n",
        "    output = self.dense(context_layer)\n",
        "\n",
        "    # print(output.shape)\n",
        "    if use_cache:\n",
        "        # Can be simplified, but I didn't for readability's sake\n",
        "        if has_cache:\n",
        "            output = torch.cat((self.past_output, output), dim=-2)\n",
        "            self.past_output = output\n",
        "        else:\n",
        "            self.past_output = output\n",
        "        has_cache = True \n",
        "    output = self.output_dropout(output)\n",
        "    return output, has_cache\n",
        "\n",
        "\n",
        "def dalle_mlp_forward(self, hidden_states, has_cache=False, use_cache=False):\n",
        "    if has_cache and use_cache:\n",
        "        hidden_states = hidden_states[:, self.past_x.shape[1]:]\n",
        "\n",
        "    # [b, s, 4hp]\n",
        "    x = self.dense_h_to_4h(hidden_states)\n",
        "    x = gelu(x)\n",
        "    # [b, s, h]\n",
        "    x = self.dense_4h_to_h(x)\n",
        "    if use_cache:\n",
        "        # Can be simplified, but isn't for readability's sake\n",
        "        if has_cache:\n",
        "            x = torch.cat((self.past_x, x), dim=-2)\n",
        "            self.past_x = x\n",
        "        else:\n",
        "            self.past_x = x\n",
        "        has_cache = True\n",
        "    else:\n",
        "        has_cache = False\n",
        "    output = self.dropout(x)\n",
        "    return output, has_cache\n",
        "\n",
        "\n",
        "# Speeds up like 6 seconds.\n",
        "def ln_forward(self, input, has_cache=False, use_cache=False):\n",
        "    if has_cache and use_cache:\n",
        "        input = input[:, self.past_output.shape[1]:]\n",
        "    \n",
        "    output = F.layer_norm(\n",
        "        input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "    \n",
        "    if use_cache:\n",
        "        # Can be simplified, but isn't readability's sake\n",
        "        if has_cache:\n",
        "            output = torch.cat((self.past_output, output), dim=1)\n",
        "            self.past_output = output\n",
        "        else:\n",
        "            self.past_output = output\n",
        "        has_cache = True\n",
        "    else:\n",
        "        has_cache = False\n",
        "    return output\n",
        "\n",
        "\n",
        "import inspect\n",
        "from functools import partial\n",
        "\n",
        "for layer in dalle.module.transformer.layers:\n",
        "    layer.forward = partial(dalle_layer_forward, layer)\n",
        "    layer.mlp.forward = partial(dalle_mlp_forward, layer.mlp)\n",
        "    layer.attention.past_attentions = None\n",
        "    layer.attention.past_query = None\n",
        "    layer.attention.forward = partial(dalle_sa_forward, layer.attention)\n",
        "    for ln in [layer.input_layernorm,\n",
        "               layer.before_first_addition_layernorm,\n",
        "               layer.post_attention_layernorm,\n",
        "               layer.before_second_addition_layernorm]:\n",
        "        ln.forward = partial(ln_forward, ln)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#--------------------\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "op(c.ok, 'Setup finished.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ruDALLE generation\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "generate_image_of = '' #@param {type:\"string\"}\n",
        "output_dir = '' #@param {type:\"string\"}\n",
        "seed =  0#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Total number of images produced is images_per_cycle * num_cycles.\n",
        "#@markdown For free tier, do not exceed 3 images per cycle.\n",
        "images_per_cycle =  3#@param {type:\"integer\"}\n",
        "num_cycles = 3#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown top_k_ and top_p_ control the number of tokens to be considered by probability and count.\n",
        "#@markdown It's recommended that you use the defaults here\n",
        "top_k_ =  1024#@param {type:\"integer\"}\n",
        "top_p_ =  .995#@param {type:\"number\"}\n",
        "\n",
        "abs_root_path = fix_path(drive_root+output_dir)\n",
        "\n",
        "if '://' in generate_image_of:\n",
        "  iterator = range(0,100)\n",
        "  use_api = True\n",
        "else:\n",
        "  iterator = generate_image_of.split(';')\n",
        "  use_api = False\n",
        "\n",
        "total = len(iterator)\n",
        "\n",
        "for txt_index, title in enumerate(iterator):\n",
        "\n",
        "  if use_api is True:\n",
        "    title = requests.get(generate_image_of+'/?i='+str(txt_index)).text\n",
        "\n",
        "  text = translate(title, 'ru')\n",
        "\n",
        "  if seed is 0:\n",
        "    random.seed()\n",
        "    random_seed = random.randint(0, 2**32)\n",
        "  else:\n",
        "    random_seed = seed\n",
        "\n",
        "  seed_everything(random_seed)\n",
        "\n",
        "  file_title = ''.join(e for e in title[:120].title() if e.isalnum())\n",
        "\n",
        "  hash_val = str(hash(text + str(top_k_) + str(top_p_)))[-5:]\n",
        "\n",
        "  output.clear()\n",
        "  op(c.title, str(txt_index+1)+'/'+str(total)+' '+': '+str(images_per_cycle*num_cycles)+' images')\n",
        "  op(c.title, 'Prompt', title)\n",
        "  op(c.title, 'Actual', text)\n",
        "\n",
        "  imageIndex = 0\n",
        "  for top_k, top_p, images_num in tqdm([(top_k_, top_p_, images_per_cycle),]*num_cycles):\n",
        "      pil_images = []\n",
        "      scores = []\n",
        "      _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, top_p=top_p)\n",
        "      show([pil_image for pil_image, score in sorted(zip(_pil_images, _scores), key=lambda x: -x[1])], num_cycles * images_per_cycle)\n",
        "      pil_images += _pil_images\n",
        "      scores += _scores\n",
        "\n",
        "      for i in range(images_per_cycle):\n",
        "        sr_images = super_resolution([pil_images[i]], realesrgan)\n",
        "        filename = abs_root_path + f\"/{hash_val}-{random_seed}{imageIndex:04}_{file_title}.png\"\n",
        "        imageIndex += 1\n",
        "        show(sr_images, 1, filename = filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NdyndHs7wJAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}